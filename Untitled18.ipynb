{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzg3r+eLP4Bruv9suX2YOZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GOKULSINGHSHAH123/Essay-writer-multiAi-agent/blob/main/Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vQBDssJb7KLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** To set up the necessary environment for this project, we need to install several Python packages. Use the following commands to install the required dependencies:**"
      ],
      "metadata": {
        "id": "aBaQ8iU460sO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GevwREW4o79"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai  # This package provides access to Google’s generative AI models, enabling advanced AI functionalities in your project.\n",
        "!pip install langchain-google-genai # This package integrates LangChain with Google's generative AI models, allowing for seamless interaction between LangChain and Google's AI services.\n",
        "!pip install langchain # LangChain is a framework for developing applications powered by language models. It provides tools and abstractions for working with LLMs.\n",
        "!pip install langchain_community # This package includes additional features and community-driven extensions for LangChain.\n",
        "!pip install langchain_openai #This package integrates LangChain with OpenAI's models, enabling enhanced functionality with OpenAI’s language models.\n",
        "!pip install tavily-python # Tavily Python package provides access to the Tavily API, which may be used for specific integrations or functionalities within the project."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following Python code snippet demonstrates the setup of a state management system using the LangGraph and LangChain Google Generative AI libraries, along with basic type annotations and imports:**"
      ],
      "metadata": {
        "id": "jzHkOubJ7vsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "stX4qvhK7vno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END  #StateGraph, END: Imported from langgraph.graph. StateGraph is used to manage and track states within a graph-based framework,\n",
        "                                             #while END represents a termination state in the graph.\n",
        "\n",
        "from typing import TypedDict, Annotated, List #TypedDict, Annotated, List: Imported from the typing module. These provide type hints for dictionary-like objects, annotated types, and lists respectively.\n",
        "\n",
        "import operator #  A standard library module providing a set of functions that correspond to the intrinsic operators of Python.\n",
        "\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver # SqliteSaver: Imported from langgraph.checkpoint.sqlite. This class is used to save and manage the state of the graph in an SQLite database.\n",
        "\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage #AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage: Imported from langchain_core.messages.\n",
        "                                                    #These classes define different types of messages used in a conversational AI context.\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI #ChatGoogleGenerativeAI: Imported from langchain_google_genai. This class provides access to Google’s generative AI capabilities for chat-based interactions.\n",
        "\n",
        "from langchain_openai import ChatOpenAI  #ChatOpenAI: Imported from langchain_openai. This class is typically used to interact with OpenAI's chat models, providing an interface for generating conversational responses"
      ],
      "metadata": {
        "id": "OUgFoa_I4rd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**memory: An instance of SqliteSaver initialized with an in-memory SQLite database (\":memory:\"). This setup is typically used for temporary storage and testing purposes, where the database resides in RAM and is not persisted to disk.\n",
        "**"
      ],
      "metadata": {
        "id": "idrQJj6B8hU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = SqliteSaver.from_conn_string(\":memory:\")"
      ],
      "metadata": {
        "id": "kSK2bZnG8bxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The AgentState class is a TypedDict that defines the structure for storing the state of an agent in a typed dictionary format. This class is used to represent various attributes related to the agent's task management and content revision process.**"
      ],
      "metadata": {
        "id": "KKzQ0IWY8pvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    task: str\n",
        "    plan: str\n",
        "    draft: str\n",
        "    critique: str\n",
        "    content: List[str]\n",
        "    revision_number: int\n",
        "    max_revisions: int"
      ],
      "metadata": {
        "id": "NZ7HA2ZZ4_nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining gemini model **"
      ],
      "metadata": {
        "id": "xE-xZiHB89fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"AIzaSyAxaNMQlai9ybEiAVlSIOaY-pRivTMuR54\", convert_system_message_to_human=True, temperature=0.3)\n",
        "\n",
        "from tavily import TavilyClient\n",
        "import os\n",
        "tavily = TavilyClient(api_key=\"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\")\n"
      ],
      "metadata": {
        "id": "-xK6kQlM5JvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt for different agents**"
      ],
      "metadata": {
        "id": "J8ToNiKu9YtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
        "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
        "or instructions for the sections.\"\"\"\n",
        "\n",
        "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
        "Generate the best essay possible for the user's request and the initial outline. \\\n",
        "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
        "Utilize all the information below as needed:\n",
        "\n",
        "------\n",
        "\n",
        "{content}\"\"\"\n",
        "\n",
        "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
        "Generate critique and recommendations for the user's submission. \\\n",
        "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
        "\n",
        "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
        "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
        "any relevant information. Only generate 3 queries max.\"\"\"\n",
        "\n",
        "\n",
        "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
        "be used when making any requested revisions (as outlined below). \\\n",
        "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n"
      ],
      "metadata": {
        "id": "k8f23bkL5nC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Queries class is a data model defined using the BaseModel class from the langchain_core.pydantic_v1 module. It is designed to represent a collection of queries in a structured format.**"
      ],
      "metadata": {
        "id": "RfrEn0lN9sV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "\n",
        "class Queries(BaseModel):  # Here Base model will keep check if the queries are list of string\n",
        "    queries: List[str]\n"
      ],
      "metadata": {
        "id": "wdcNGMHK6cEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Travily for search to browser*"
      ],
      "metadata": {
        "id": "rJlVdmI3_zwF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPcE6EVr8KQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_NYQ1m3ZE66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The plan_node function is designed to interact with a generative AI model to create a plan based on the current state of an agent. This function utilizes system and human messages to guide the model in generating a response.**"
      ],
      "metadata": {
        "id": "Ry82fI5rAcMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plan_node(state: AgentState):         # It will act like a planing agent that will gi an ove utline of the essay along with any relevant notes\n",
        "                                            #or instructions for the sections\n",
        "    print(\"reached planner node\")\n",
        "    messages = [\n",
        "        SystemMessage(content=PLAN_PROMPT),   #SystemMessage: Includes a predefined prompt (PLAN_PROMPT) that instructs the model on how to generate the plan.\n",
        "        HumanMessage(content=state['task'])    #HumanMessage: Contains the current task from the state which is used as input to the model\n",
        "    ]\n",
        "    print(\"hi\",messages)\n",
        "    response = model.invoke(messages)           #generate response\n",
        "    print(\"hi\",response)\n",
        "\n",
        "    return {\"plan\": response.content}           #return the plan"
      ],
      "metadata": {
        "id": "I3VHu24b8_q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def research_plan_node(state: AgentState):\n",
        "#     print(\"reached research_plan_node\")\n",
        "#     queries = model.with_structured_output(Queries)\n",
        "#     print(\"passed 1\")\n",
        "#     messages = [\n",
        "#         SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
        "#         HumanMessage(content=state['task'])\n",
        "#     ]\n",
        "#     print(\"passed 2\")\n",
        "#     queries = queries.invoke(messages)\n",
        "#     print(\"passed 3\")\n",
        "\n",
        "#     print(\"hi\",queries)\n",
        "#     content = state['content'] or []\n",
        "#     for q in queries.queries:\n",
        "#         response = tavily.search(query=q, max_results=2)\n",
        "#         for r in response['results']:\n",
        "#             content.append(r['content'])\n",
        "#     return {\"content\": content}"
      ],
      "metadata": {
        "id": "CG4yaMrh9-U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The research_plan_node function act like research planning agent that generates research queries based on the agent's task and retrieves relevant content using those queries. It interacts with a generative AI model and a search service to accomplish this.**"
      ],
      "metadata": {
        "id": "CP57-3brD8P3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e6999sttEEny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def research_plan_node(state: AgentState):\n",
        "    print(\"reached research_plan_node\")\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=RESEARCH_PLAN_PROMPT),    #Again defining the message\n",
        "        HumanMessage(content=state['task'])\n",
        "    ]\n",
        "    print(\"passed 1\")\n",
        "\n",
        "    try:\n",
        "        response = model.invoke(messages)             # Now getting respense from model\n",
        "        print(type(response))\n",
        "        print(\"list of queries\", response)\n",
        "        print(\"passed 2\")\n",
        "\n",
        "        # Extract the text content from the response\n",
        "        response_text = response.content if hasattr(response, 'content') else str(response)   # Extracting the required text from response\n",
        "\n",
        "        # Parse the queries from the text content\n",
        "        queries_list = parse_queries_from_content(response_text)      # now we want the response to be in list format so i pass the function to parse_queries_from_conten function\n",
        "        print(\"parsed queries list\", queries_list)                    #where it will return the list\n",
        "\n",
        "        # Perform search based on extracted queries\n",
        "        content = state['content'] or []\n",
        "        for q in queries_list:                                        #now the list we have ,they have queries for the essay that will search in browser by tavily in loop max turns 2\n",
        "            response = tavily.search(query=q, max_results=2)          #now appending the data comes from search to content\n",
        "            for r in response['results']:\n",
        "                content.append(r['content'])\n",
        "\n",
        "        return {\"content\": content}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return {\"content\": state['content']}\n",
        "\n",
        "def parse_queries_from_content(content: str):\n",
        "    # Split the content by newlines\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    # Filter out empty lines and extract queries\n",
        "    queries = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # Remove numbering and quotes if needed\n",
        "    queries = [q.split('. ', 1)[-1].strip('\"') for q in queries]\n",
        "\n",
        "    return queries\n"
      ],
      "metadata": {
        "id": "38YECaysCq90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The generation_node function generates a draft of content based on the agent's task, plan, and any previously researched content. It interacts with a generative AI model to produce the draft.**"
      ],
      "metadata": {
        "id": "hSVGxpvqO-BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generation_node(state: AgentState):\n",
        "    print(\"reached generation_node\")\n",
        "    content = \"\\n\\n\".join(state['content'] or [])\n",
        "    user_message = HumanMessage(\n",
        "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
        "    messages = [                                                                  # Now the content that is researched by research agent  and the plan of the essay generated by\n",
        "        SystemMessage(                                                            # planer agent will be used to generate essay using gneration agent\n",
        "            content=WRITER_PROMPT.format(content=content)\n",
        "        ),\n",
        "        user_message\n",
        "        ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\n",
        "        \"draft\": response.content,                               #updating draft\n",
        "        \"revision_number\": state.get(\"revision_number\", 1) + 1   #updating revision number\n",
        "    }"
      ],
      "metadata": {
        "id": "L7S8EIVV-QTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The reflection_node function generates a critique or reflection on the draft content produced by the generation node. It uses a generative AI model to analyze and provide recommendations for the draft.**"
      ],
      "metadata": {
        "id": "e-m9UnXAPEW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reflection_node(state: AgentState):\n",
        "    print(\"reached reflection_node\")\n",
        "    messages = [\n",
        "        SystemMessage(content=REFLECTION_PROMPT),       #Now generating reflection and recommendation on essay generated by generation code\n",
        "        HumanMessage(content=state['draft'])\n",
        "    ]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"critique\": response.content}               #updatig critique"
      ],
      "metadata": {
        "id": "3gP0HOPEEBml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The research_critique_node function generates additional research content based on the critique provided by the reflection node. It performs a search for content related to the critique to enrich the agent's knowledge.**"
      ],
      "metadata": {
        "id": "fELZcaDyPHCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def research_critique_node(state: AgentState):\n",
        "    print(\"reached research_critique_node\")\n",
        "\n",
        "    # Send messages to the model without structured output\n",
        "    messages = [\n",
        "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
        "        HumanMessage(content=state['critique'])\n",
        "    ]\n",
        "    print(\"passed 1\")\n",
        "\n",
        "    try:\n",
        "        response = model.invoke(messages)\n",
        "        print(\"passed 2\")                                                               #Again doinfg the research by taking the recommendation from reflectioon agent and searching for\n",
        "                                                                                        #content like done in research agent\n",
        "        # Extract content from the response\n",
        "        response_content = response.content if hasattr(response, 'content') else str(response)\n",
        "        print(\"queries are\", response_content)\n",
        "\n",
        "        # Parse queries from the response content\n",
        "        queries_list = parse_queries_from_text(response_content)\n",
        "        print(\"queries_list\", queries_list)\n",
        "\n",
        "        # Perform search based on extracted queries\n",
        "        content = state['content'] or []\n",
        "        for q in queries_list:\n",
        "            response = tavily.search(query=q, max_results=2)\n",
        "            for r in response['results']:\n",
        "                content.append(r['content'])\n",
        "\n",
        "        return {\"content\": content}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return {\"content\": state['content']}\n",
        "\n",
        "def parse_queries_from_text(text):\n",
        "    print(text)\n",
        "    # Assuming the queries are separated by new lines and potentially prefixed by numbers or bullet points\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Clean up and filter out empty lines\n",
        "    queries = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # If needed, remove numbering or bullet points\n",
        "    queries = [q.split('. ', 1)[-1].strip('\"') for q in queries]\n",
        "\n",
        "    return queries\n"
      ],
      "metadata": {
        "id": "9Wu-eXRYD2cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def research_critique_node(state: AgentState):\n",
        "#     print(\"reached research_critique_node\")\n",
        "#     queries = model.with_structured_output(Queries).invoke([\n",
        "#         SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
        "#         HumanMessage(content=state['critique'])\n",
        "#     ])\n",
        "#     content = state['content'] or []\n",
        "#     for q in queries.queries:\n",
        "#         response = tavily.search(query=q, max_results=2)\n",
        "#         for r in response['results']:\n",
        "#             content.append(r['content'])\n",
        "#     return {\"content\": content}"
      ],
      "metadata": {
        "id": "lzuAMDU9HVvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**he should_continue function determines whether the process should continue based on the current revision number and the maximum allowed revisions. It helps control the workflow by deciding when to end the process or proceed to the reflection stage.**"
      ],
      "metadata": {
        "id": "0-PXLQxBPTQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state):\n",
        "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
        "        return END\n",
        "    return \"reflect\""
      ],
      "metadata": {
        "id": "_7Y0hNPCHZ7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following line of code initializes an instance of the StateGraph class, which is used to manage the state transitions and workflows for the agent system**"
      ],
      "metadata": {
        "id": "FpEdm81VPd0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(AgentState)"
      ],
      "metadata": {
        "id": "aSggunqCHb0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qtca5xJYPm9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following code adds nodes to the StateGraph instance. Each node represents a specific state or action in the workflow managed by the StateGraph.**"
      ],
      "metadata": {
        "id": "Xc5yLrQGPk2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder.add_node(\"planner\", plan_node)\n",
        "builder.add_node(\"generate\", generation_node)\n",
        "builder.add_node(\"reflect\", reflection_node)\n",
        "builder.add_node(\"research_plan\", research_plan_node)\n",
        "builder.add_node(\"research_critique\", research_critique_node)"
      ],
      "metadata": {
        "id": "gk3fRdnG1dhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder.set_entry_point(\"planner\")         # setting planner to be starting node"
      ],
      "metadata": {
        "id": "d8Rk3K0S1f4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder.add_conditional_edges(\n",
        "    \"generate\",                           # Checking condition if essay should go for reflection or should be completed\n",
        "    should_continue,\n",
        "    {END: END, \"reflect\": \"reflect\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "QEQmGQB01i1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following code adds edges to the StateGraph instance. Each edge defines a transition from one node (state) to another, specifying the workflow of the agent system.**"
      ],
      "metadata": {
        "id": "Ue7LUG3pP47k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder.add_edge(\"planner\", \"research_plan\")\n",
        "builder.add_edge(\"research_plan\", \"generate\")\n",
        "builder.add_edge(\"reflect\", \"research_critique\")\n",
        "builder.add_edge(\"research_critique\", \"generate\")"
      ],
      "metadata": {
        "id": "vxbUFnLV1k60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = builder.compile(checkpointer=memory)  # compiling the graph"
      ],
      "metadata": {
        "id": "MPumyD0y1m5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following code demonstrates how to execute a streaming process from the StateGraph instance. This process involves running a workflow with a specified initial state and configuration, and processing the results in real-time.**"
      ],
      "metadata": {
        "id": "T36eybPBQQcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "for s in graph.stream({\n",
        "    'task': \"what is the difference between langchain and langsmith\",\n",
        "    \"max_revisions\": 2,\n",
        "    \"revision_number\": 1,\n",
        "}, thread):\n",
        "    print(s)"
      ],
      "metadata": {
        "id": "Fb40idTG1qIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***USING GRADIO FOR UI AND DEPLOYMENT***"
      ],
      "metadata": {
        "id": "ZRVt8X2ZQY6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "wIBzM7Z2OHez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "graph = builder.compile(checkpointer=SqliteSaver.from_conn_string(\":memory:\"))\n",
        "\n",
        "# Define functions for Gradio\n",
        "def process_essay(task, max_revisions):\n",
        "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "    results = []\n",
        "    for s in graph.stream({\n",
        "        'task': task,\n",
        "        \"max_revisions\": max_revisions,\n",
        "        \"revision_number\": 1,\n",
        "    }, thread):\n",
        "        results.append(s)\n",
        "    return results\n",
        "\n",
        "def extract_agent_outputs(results):\n",
        "    plan_output = \"\"\n",
        "    research_output = \"\"\n",
        "    generation_output = \"\"\n",
        "    reflection_output = \"\"\n",
        "    final_essay = \"\"\n",
        "\n",
        "    for step in results:\n",
        "        print(\"Processing step:\", step)  # Debug line\n",
        "        if 'planner' in step:\n",
        "            plan_output += f\"Plan:\\n{step['planner']['plan']}\\n\\n\"\n",
        "        if 'research_plan' in step or 'research_critique' in step:\n",
        "            research_key = 'research_plan' if 'research_plan' in step else 'research_critique'\n",
        "            if step[research_key]:\n",
        "                research_output += f\"Research Queries:\\n{step[research_key]['content']}\\n\\n\"\n",
        "            else:\n",
        "                research_output += f\"Research Queries:\\nNo content found.\\n\\n\"\n",
        "        if 'generate' in step:\n",
        "            generation_output += f\"Draft (Revision {step['generate']['revision_number']}):\\n{step['generate']['draft']}\\n\\n\"\n",
        "            final_essay = step['generate']['draft']  # Update final essay\n",
        "        if 'reflect' in step:\n",
        "            if step['reflect']:\n",
        "                reflection_output += f\"Critique:\\n{step['reflect']['critique']}\\n\\n\"\n",
        "            else:\n",
        "                reflection_output += f\"Critique:\\nNo critique found.\\n\\n\"\n",
        "\n",
        "    return plan_output, research_output, generation_output, reflection_output, final_essay\n",
        "\n",
        "def gradio_interface(task, max_revisions):\n",
        "    results = process_essay(task, max_revisions)\n",
        "    plan, research, generation, reflection, final_essay = extract_agent_outputs(results)\n",
        "    return final_essay, plan, research, generation, reflection\n",
        "\n",
        "# Create and launch Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Essay Topic\", placeholder=\"Enter the essay topic or task here\"),\n",
        "        gr.Slider(minimum=1, maximum=5, step=1, label=\"Max Revisions\", value=2)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Final Essay\"),\n",
        "        gr.Textbox(label=\"Planning Output\", lines=10),\n",
        "        gr.Textbox(label=\"Research Output\", lines=10),\n",
        "        gr.Textbox(label=\"Generation Output\", lines=10),\n",
        "        gr.Textbox(label=\"Reflection Output\", lines=10)\n",
        "    ],\n",
        "    title=\"Multi-Agent Essay Writing System\",\n",
        "    description=\"This system uses multiple AI agents to plan, research, write, and revise an essay on the given topic.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "lFtR45YUr6bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3emNb1hyLZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}